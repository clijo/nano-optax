{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Nano Optax","text":"<p>Implementation of classic optimization algorithms in JAX.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>uv add nano-optax\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom nano_optax import sgd, step_lr\n\n# 1. Setup Data\nkey = jax.random.PRNGKey(0)\nX = jax.random.normal(key, (100, 1))\ntrue_w = jnp.array([2.5])\nnoise = 0.01 * jax.random.normal(key, (100,))\ny = X @ true_w + noise\n\ndata = (X, y)\n\n# 2. Define Objective\ndef fun(params, x, y):\n    pred = x @ params[\"w\"]\n    return jnp.mean((pred - y) ** 2)\n\n# 3. Minimize\ninit_params = {\"w\": jnp.array([0.0])}\nresult = sgd(\n    fun,\n    init_params,\n    data,\n    lr=step_lr(base_lr=0.1, step_size=1000, gamma=0.5),\n    batch_size=16,\n    key=jax.random.PRNGKey(42),\n)\nprint(result.params)\n</code></pre>"},{"location":"#schedulers","title":"Schedulers","text":"<p>See Schedulers for learning-rate schedule utilities and examples.</p>"},{"location":"schedulers/","title":"Schedulers","text":"<p>Learning-rate schedules in <code>nano-optax</code> are pure functions of step. You can pass any callable <code>schedule(step) -&gt; lr</code> to a solver, or use the helpers below.</p>"},{"location":"schedulers/#quick-usage","title":"Quick Usage","text":"<pre><code>from nano_optax import sgd, step_lr\n\nschedule = step_lr(base_lr=0.1, step_size=1000, gamma=0.5)\nresult = sgd(fun, init_params, data, lr=schedule, batch_size=16)\n</code></pre> <p><code>step_lr</code> counts minibatch steps, not epochs. If you want to decay every <code>N</code> epochs, set <code>step_size = N * batches_per_epoch</code>.</p>"},{"location":"schedulers/#built-in-schedulers","title":"Built-in Schedulers","text":"<ul> <li><code>constant_lr</code>: fixed learning rate.</li> <li><code>lambda_lr</code>: user-defined schedule function.</li> <li><code>step_lr</code>: multiplicative decay every <code>step_size</code> steps.</li> </ul>"},{"location":"schedulers/#lambda-example","title":"Lambda Example","text":"<pre><code>import jax.numpy as jnp\nfrom nano_optax import lambda_lr\n\nschedule = lambda_lr(lambda step: jnp.exp(-0.001 * step))\n</code></pre>"},{"location":"schedulers/#stateful-schedule-example","title":"Stateful Schedule Example","text":"<p>If you need schedules that depend on previous values, pass a stateful schedule function <code>(step, state) -&gt; (lr, new_state)</code> and an initial <code>schedule_state</code>:</p> <pre><code>import jax.numpy as jnp\n\ndef adaptive_schedule(step, state):\n    lr = state[\"lr\"]\n    new_lr = lr * jnp.where(step % 100 == 0, 0.5, 1.0)\n    return lr, {\"lr\": new_lr}\n</code></pre> <p>Use it by passing <code>schedule_state</code> to the solver.</p>"},{"location":"schedulers/#api","title":"API","text":""},{"location":"schedulers/#nano_optax.schedulers.constant_lr","title":"constant_lr","text":"<pre><code>constant_lr(lr: float | Array) -&gt; Callable[[Array], Array]\n</code></pre> <p>Return a constant learning-rate schedule.</p> Source code in <code>src/nano_optax/schedulers.py</code> <pre><code>def constant_lr(lr: float | jax.Array) -&gt; Callable[[jax.Array], jax.Array]:\n    \"\"\"Return a constant learning-rate schedule.\"\"\"\n    lr_val = jnp.asarray(lr)\n\n    def schedule(step: jax.Array) -&gt; jax.Array:  # noqa: ARG001\n        return lr_val\n\n    return schedule\n</code></pre>"},{"location":"schedulers/#nano_optax.schedulers.lambda_lr","title":"lambda_lr","text":"<pre><code>lambda_lr(\n    lr_lambda: Callable[[Array], Array],\n) -&gt; Callable[[Array], Array]\n</code></pre> <p>Schedule defined by a user-provided callable.</p> Source code in <code>src/nano_optax/schedulers.py</code> <pre><code>def lambda_lr(\n    lr_lambda: Callable[[jax.Array], jax.Array],\n) -&gt; Callable[[jax.Array], jax.Array]:\n    \"\"\"Schedule defined by a user-provided callable.\"\"\"\n\n    def schedule(step: jax.Array) -&gt; jax.Array:\n        return lr_lambda(step)\n\n    return schedule\n</code></pre>"},{"location":"schedulers/#nano_optax.schedulers.step_lr","title":"step_lr","text":"<pre><code>step_lr(\n    base_lr: float | Array,\n    step_size: int,\n    gamma: float = 0.1,\n) -&gt; Callable[[Array], Array]\n</code></pre> <p>Decay learning rate by gamma every <code>step_size</code> steps.</p> Source code in <code>src/nano_optax/schedulers.py</code> <pre><code>def step_lr(\n    base_lr: float | jax.Array,\n    step_size: int,\n    gamma: float = 0.1,\n) -&gt; Callable[[jax.Array], jax.Array]:\n    \"\"\"Decay learning rate by gamma every `step_size` steps.\"\"\"\n    if step_size &lt;= 0:\n        raise ValueError(\"step_size must be positive\")\n    base_lr = jnp.asarray(base_lr)\n    gamma_val = jnp.asarray(gamma)\n\n    def schedule(step: jax.Array) -&gt; jax.Array:\n        exponent = jnp.floor_divide(step, step_size)\n        return base_lr * jnp.power(gamma_val, exponent)\n\n    return schedule\n</code></pre>"},{"location":"schedulers/#nano_optax.schedulers.as_schedule","title":"as_schedule","text":"<pre><code>as_schedule(\n    lr: LearningRate,\n    schedule_state: ScheduleState | None = None,\n) -&gt; tuple[ScheduleFn, ScheduleState | None]\n</code></pre> <p>Normalize to a pure schedule function with explicit state.</p> <p>Returns a function <code>(step, state) -&gt; (lr, new_state)</code> and the initial state. If the schedule is stateless, the state is passed through unchanged. The schedule state must be a JAX PyTree to be compatible with JIT/scan.</p> Source code in <code>src/nano_optax/schedulers.py</code> <pre><code>def as_schedule(\n    lr: LearningRate,\n    schedule_state: ScheduleState | None = None,\n) -&gt; tuple[ScheduleFn, ScheduleState | None]:\n    \"\"\"Normalize to a pure schedule function with explicit state.\n\n    Returns a function `(step, state) -&gt; (lr, new_state)` and the initial state.\n    If the schedule is stateless, the state is passed through unchanged. The\n    schedule state must be a JAX PyTree to be compatible with JIT/scan.\n    \"\"\"\n    if callable(lr):\n        if schedule_state is None:\n            stateless = cast(Callable[[jax.Array], jax.Array], lr)\n\n            def scheduler(\n                step: jax.Array, state: ScheduleState | None\n            ) -&gt; tuple[jax.Array, ScheduleState | None]:\n                return stateless(step), state\n\n            return scheduler, schedule_state\n\n        stateful = cast(\n            Callable[[jax.Array, ScheduleState], tuple[jax.Array, ScheduleState]], lr\n        )\n\n        def scheduler(\n            step: jax.Array, state: ScheduleState\n        ) -&gt; tuple[jax.Array, ScheduleState]:\n            lr_val, new_state = stateful(step, state)\n            return lr_val, new_state\n\n        return scheduler, schedule_state\n\n    lr_val = jnp.asarray(lr)\n\n    def scheduler(\n        step: jax.Array, state: ScheduleState | None\n    ) -&gt; tuple[jax.Array, ScheduleState | None]:\n        return lr_val, state\n\n    return scheduler, schedule_state\n</code></pre>"},{"location":"solvers/","title":"Solvers","text":"<p>All solvers in <code>nano-optax</code> are pure functions. Each solver takes an objective <code>f(params, *data)</code> and returns an <code>OptResult</code> with final parameters, final objective value, and a per-epoch trace.</p> <pre><code>from nano_optax import gd\n\nresult = gd(fun, init_params, data, lr=1e-2, max_epochs=100)\n</code></pre> <p>If you want to use a stateful schedule, pass a schedule function with signature <code>(step, state) -&gt; (lr, new_state)</code> and provide <code>schedule_state</code>.</p>"},{"location":"solvers/#gradient-descent","title":"Gradient Descent","text":""},{"location":"solvers/#nano_optax.gd.gd","title":"gd","text":"<pre><code>gd(\n    fun: Callable[..., Array],\n    init_params: PyTree,\n    data: tuple = (),\n    *,\n    lr: LearningRate = 0.001,\n    max_epochs: int = 100,\n    tol: float = 1e-06,\n    schedule_state: ScheduleState | None = None,\n    verbose: bool = False,\n) -&gt; OptResult\n</code></pre> <p>Run vanilla gradient descent.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable[..., Array]</code> <p>Objective function <code>f(params, *data) -&gt; value</code>.</p> required <code>init_params</code> <code>PyTree</code> <p>Initial parameters (PyTree).</p> required <code>data</code> <code>tuple</code> <p>Tuple of data arrays.</p> <code>()</code> <code>lr</code> <code>LearningRate</code> <p>Learning rate (constant, schedule, or stateful schedule).</p> <code>0.001</code> <code>max_epochs</code> <code>int</code> <p>Number of epochs to run.</p> <code>100</code> <code>tol</code> <code>float</code> <p>Convergence tolerance on gradient norm.</p> <code>1e-06</code> <code>schedule_state</code> <code>ScheduleState | None</code> <p>Optional initial state for a stateful schedule.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress during optimization.</p> <code>False</code> <p>Returns:</p> Type Description <code>OptResult</code> <p>OptResult with final parameters, value, and trace.</p> Source code in <code>src/nano_optax/gd.py</code> <pre><code>def gd(\n    fun: Callable[..., jax.Array],\n    init_params: PyTree,\n    data: tuple = (),\n    *,\n    lr: LearningRate = 1e-3,\n    max_epochs: int = 100,\n    tol: float = 1e-6,\n    schedule_state: ScheduleState | None = None,\n    verbose: bool = False,\n) -&gt; OptResult:\n    \"\"\"Run vanilla gradient descent.\n\n    Args:\n        fun: Objective function `f(params, *data) -&gt; value`.\n        init_params: Initial parameters (PyTree).\n        data: Tuple of data arrays.\n        lr: Learning rate (constant, schedule, or stateful schedule).\n        max_epochs: Number of epochs to run.\n        tol: Convergence tolerance on gradient norm.\n        schedule_state: Optional initial state for a stateful schedule.\n        verbose: Print progress during optimization.\n\n    Returns:\n        OptResult with final parameters, value, and trace.\n    \"\"\"\n    scheduler, schedule_state = as_schedule(lr, schedule_state)\n    tol_val = jnp.asarray(tol)\n\n    init_val = fun(init_params, *data)\n\n    init_state = GDState(\n        params=init_params,\n        schedule_state=schedule_state,\n        step=jnp.array(0, dtype=jnp.int32),\n        value=init_val,\n        converged=jnp.array(False, dtype=jnp.bool_),\n    )\n\n    def scan_body(carry: GDState, _):\n        params, sched_state, step, prev_val, converged = carry\n\n        def perform_step(operand):\n            p, s_state, s = operand\n            lr_val, new_s_state = scheduler(s, s_state)\n            val, grads = jax.value_and_grad(fun)(p, *data)\n\n            sq_norm_grads = jax.tree_util.tree_reduce(\n                jnp.add, jax.tree.map(lambda g: jnp.sum(g**2), grads)\n            )\n            grad_norm = jnp.sqrt(sq_norm_grads)\n\n            new_p = jax.tree.map(lambda p_i, g_i: p_i - lr_val * g_i, p, grads)\n\n            just_converged = grad_norm &lt; tol_val\n            final_p = jax.tree.map(\n                lambda old, new: jnp.where(just_converged, old, new), p, new_p\n            )\n\n            return final_p, new_s_state, s + 1, val, just_converged\n\n        def skip_step(operand):\n            p, s_state, s = operand\n            return p, s_state, s, prev_val, jnp.array(True, dtype=jnp.bool_)\n\n        new_params, new_sched_state, new_step, new_val, now_converged = jax.lax.cond(\n            converged,\n            skip_step,\n            perform_step,\n            (params, sched_state, step),\n        )\n\n        new_state = GDState(\n            params=new_params,\n            schedule_state=new_sched_state,\n            step=new_step,\n            value=new_val,\n            converged=now_converged,\n        )\n\n        if verbose:\n            jax.debug.print(\"Epoch {}: value={}\", new_step, new_val)\n\n        return new_state, new_val\n\n    final_state, trace = jax.lax.scan(scan_body, init_state, None, length=max_epochs)\n    final_value = fun(final_state.params, *data)\n\n    return OptResult(\n        params=final_state.params,\n        final_value=final_value,\n        trace=trace,\n        success=final_state.converged,\n    )\n</code></pre>"},{"location":"solvers/#stochastic-gradient-descent","title":"Stochastic Gradient Descent","text":""},{"location":"solvers/#nano_optax.sgd.sgd","title":"sgd","text":"<pre><code>sgd(\n    fun: Callable[..., Array],\n    init_params: PyTree,\n    data: tuple[Array, ...],\n    *,\n    lr: LearningRate = 0.001,\n    max_epochs: int = 100,\n    batch_size: int | None = 1,\n    key: Array | None = None,\n    schedule_state: ScheduleState | None = None,\n    verbose: bool = False,\n) -&gt; OptResult\n</code></pre> <p>Run stochastic gradient descent.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable[..., Array]</code> <p>Objective function <code>f(params, *batch_data) -&gt; value</code>.</p> required <code>init_params</code> <code>PyTree</code> <p>Initial parameters (PyTree).</p> required <code>data</code> <code>tuple[Array, ...]</code> <p>Tuple of data arrays, sliced along axis 0.</p> required <code>lr</code> <code>LearningRate</code> <p>Learning rate (constant, schedule, or stateful schedule).</p> <code>0.001</code> <code>max_epochs</code> <code>int</code> <p>Number of epochs to run.</p> <code>100</code> <code>batch_size</code> <code>int | None</code> <p>Minibatch size (None uses full batch).</p> <code>1</code> <code>key</code> <code>Array | None</code> <p>PRNGKey for shuffling (None disables shuffling).</p> <code>None</code> <code>schedule_state</code> <code>ScheduleState | None</code> <p>Optional initial state for a stateful schedule.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress during optimization.</p> <code>False</code> <p>Returns:</p> Type Description <code>OptResult</code> <p>OptResult with final parameters, value, and trace.</p> Source code in <code>src/nano_optax/sgd.py</code> <pre><code>def sgd(\n    fun: Callable[..., jax.Array],\n    init_params: PyTree,\n    data: tuple[jax.Array, ...],\n    *,\n    lr: LearningRate = 1e-3,\n    max_epochs: int = 100,\n    batch_size: int | None = 1,\n    key: jax.Array | None = None,\n    schedule_state: ScheduleState | None = None,\n    verbose: bool = False,\n) -&gt; OptResult:\n    \"\"\"Run stochastic gradient descent.\n\n    Args:\n        fun: Objective function `f(params, *batch_data) -&gt; value`.\n        init_params: Initial parameters (PyTree).\n        data: Tuple of data arrays, sliced along axis 0.\n        lr: Learning rate (constant, schedule, or stateful schedule).\n        max_epochs: Number of epochs to run.\n        batch_size: Minibatch size (None uses full batch).\n        key: PRNGKey for shuffling (None disables shuffling).\n        schedule_state: Optional initial state for a stateful schedule.\n        verbose: Print progress during optimization.\n\n    Returns:\n        OptResult with final parameters, value, and trace.\n    \"\"\"\n    if not data:\n        raise ValueError(\"data cannot be empty for SGD.\")\n\n    num_samples = len(data[0])\n    batch_size = num_samples if batch_size is None else min(batch_size, num_samples)\n    if batch_size &lt;= 0:\n        raise ValueError(\"batch_size must be positive\")\n\n    num_full_batches = num_samples // batch_size\n    remainder = num_samples % batch_size\n\n    scheduler, schedule_state = as_schedule(lr, schedule_state)\n\n    init_state = SGDState(\n        params=init_params,\n        schedule_state=schedule_state,\n        step=jnp.array(0, dtype=jnp.int32),\n        key=key,\n        value=jnp.array(jnp.inf),\n    )\n\n    def step_fn(carry, indices):\n        params, sched_state, step_count = carry\n        batch_data = jax.tree.map(lambda x: x[indices], data)\n        lr_val, new_sched_state = scheduler(step_count, sched_state)\n\n        val, grads = jax.value_and_grad(fun)(params, *batch_data)\n        new_params = jax.tree.map(lambda p, g: p - lr_val * g, params, grads)\n\n        return (new_params, new_sched_state, step_count + 1), val\n\n    def epoch_scan(carry: SGDState, _):\n        params, sched_state, step, rng_key, _ = carry\n\n        if rng_key is not None:\n            new_key, subkey = jax.random.split(rng_key)\n            perm = jax.random.permutation(subkey, num_samples)\n        else:\n            new_key = rng_key\n            perm = jnp.arange(num_samples)\n\n        total_val = jnp.array(0.0)\n        scan_carry = (params, sched_state, step)\n\n        if num_full_batches &gt; 0:\n            full_indices = perm[: num_full_batches * batch_size].reshape(\n                (num_full_batches, batch_size)\n            )\n            scan_carry, batch_vals = jax.lax.scan(step_fn, scan_carry, full_indices)\n            total_val += jnp.sum(batch_vals) * batch_size\n\n        if remainder &gt; 0:\n            rem_indices = perm[num_full_batches * batch_size :]\n            scan_carry, val = step_fn(scan_carry, rem_indices)\n            total_val += val * remainder\n\n        new_params, new_sched_state, new_step = scan_carry\n        epoch_val = total_val / num_samples\n\n        new_state = SGDState(\n            params=new_params,\n            schedule_state=new_sched_state,\n            step=new_step,\n            key=new_key,\n            value=epoch_val,\n        )\n\n        if verbose:\n            jax.debug.print(\"Epoch {}: value={}\", new_step, epoch_val)\n\n        return new_state, epoch_val\n\n    final_state, trace = jax.lax.scan(epoch_scan, init_state, None, length=max_epochs)\n    final_value = fun(final_state.params, *data)\n\n    return OptResult(\n        params=final_state.params,\n        final_value=final_value,\n        trace=trace,\n        success=True,\n    )\n</code></pre>"},{"location":"solvers/#proximal-gradient-descent","title":"Proximal Gradient Descent","text":""},{"location":"solvers/#nano_optax.prox_gd.prox_gd","title":"prox_gd","text":"<pre><code>prox_gd(\n    fun: Callable[..., Array],\n    g: Callable[[PyTree], Array],\n    prox: Callable[[Array, Array], Array],\n    init_params: PyTree,\n    data: tuple = (),\n    *,\n    lr: LearningRate = 0.001,\n    max_epochs: int = 100,\n    tol: float = 1e-06,\n    schedule_state: ScheduleState | None = None,\n    verbose: bool = False,\n) -&gt; OptResult\n</code></pre> <p>Run proximal gradient descent for objectives of the form \\(f\\) + \\(g\\), where \\(f\\) is \\(L\\)-smooth and convex, and \\(g\\) is (possibly nonsmooth) proper, l.s.c., and convex. The proximal operator for \\(g\\) must be passed via the <code>prox</code> argument as an uncurried map with signature \\((x,\\eta)\\mapsto \\operatorname{prox}_{\\eta g}(x)\\). At iteration \\(t\\), the algorithm does a: 1. (Gradient step): \\(y_{t} := x_{t-1} - \\eta_{t}\\nabla f(x_{t-1})\\), and 2. (Proximal step) \\(x_{t}:=\\operatorname{prox}_{\\eta_{t} g}(y_{t})\\).</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable[..., Array]</code> <p>Smooth function <code>f(params, *data) -&gt; value</code>.</p> required <code>g</code> <code>Callable[[PyTree], Array]</code> <p>Nonsmooth function <code>g(params) -&gt; value</code>.</p> required <code>prox</code> <code>Callable[[Array, Array], Array]</code> <p>Proximal operator <code>prox(params, lr) -&gt; params</code>.</p> required <code>init_params</code> <code>PyTree</code> <p>Initial parameters (PyTree).</p> required <code>data</code> <code>tuple</code> <p>Tuple of data arrays.</p> <code>()</code> <code>lr</code> <code>LearningRate</code> <p>Learning rate (constant, schedule, or stateful schedule).</p> <code>0.001</code> <code>max_epochs</code> <code>int</code> <p>Number of epochs to run.</p> <code>100</code> <code>tol</code> <code>float</code> <p>Convergence tolerance on gradient mapping norm.</p> <code>1e-06</code> <code>schedule_state</code> <code>ScheduleState | None</code> <p>Optional initial state for a stateful schedule.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress during optimization.</p> <code>False</code> <p>Returns:</p> Type Description <code>OptResult</code> <p>OptResult with final parameters, value, and trace.</p> Source code in <code>src/nano_optax/prox_gd.py</code> <pre><code>def prox_gd(\n    fun: Callable[..., jax.Array],\n    g: Callable[[PyTree], jax.Array],\n    prox: Callable[[jax.Array, jax.Array], jax.Array],\n    init_params: PyTree,\n    data: tuple = (),\n    *,\n    lr: LearningRate = 1e-3,\n    max_epochs: int = 100,\n    tol: float = 1e-6,\n    schedule_state: ScheduleState | None = None,\n    verbose: bool = False,\n) -&gt; OptResult:\n    r\"\"\"Run proximal gradient descent for objectives of the form $f$ + $g$, where $f$ is $L$-smooth and convex, and $g$ is (possibly nonsmooth) proper, l.s.c., and convex. The proximal operator for $g$ must be passed via the `prox` argument as an uncurried map with signature $(x,\\eta)\\mapsto \\operatorname{prox}_{\\eta g}(x)$. At iteration $t$, the algorithm does a:\n    1. (Gradient step): $y_{t} := x_{t-1} - \\eta_{t}\\nabla f(x_{t-1})$, and\n    2. (Proximal step) $x_{t}:=\\operatorname{prox}_{\\eta_{t} g}(y_{t})$.\n\n    Args:\n        fun: Smooth function `f(params, *data) -&gt; value`.\n        g: Nonsmooth function `g(params) -&gt; value`.\n        prox: Proximal operator `prox(params, lr) -&gt; params`.\n        init_params: Initial parameters (PyTree).\n        data: Tuple of data arrays.\n        lr: Learning rate (constant, schedule, or stateful schedule).\n        max_epochs: Number of epochs to run.\n        tol: Convergence tolerance on gradient mapping norm.\n        schedule_state: Optional initial state for a stateful schedule.\n        verbose: Print progress during optimization.\n\n    Returns:\n        OptResult with final parameters, value, and trace.\n    \"\"\"\n    scheduler, schedule_state = as_schedule(lr, schedule_state)\n    tol_val = jnp.asarray(tol)\n\n    init_state = ProxGDState(\n        params=init_params,\n        schedule_state=schedule_state,\n        step=jnp.array(0, dtype=jnp.int32),\n        value=jnp.array(jnp.inf),\n        converged=jnp.array(False, dtype=jnp.bool_),\n    )\n\n    def step_fn(carry):\n        params, sched_state, step_count = carry\n        lr_val, new_sched_state = scheduler(step_count, sched_state)\n\n        val, grads = jax.value_and_grad(fun)(params, *data)\n        g_val = g(params)\n\n        new_params = jax.tree.map(\n            lambda p, gr: prox(p - lr_val * gr, lr_val), params, grads\n        )\n\n        grad_map_norm = jnp.sqrt(\n            jax.tree_util.tree_reduce(\n                jnp.add,\n                jax.tree.map(\n                    lambda p, new_p: jnp.sum(((p - new_p) / lr_val) ** 2),\n                    params,\n                    new_params,\n                ),\n            )\n        )\n\n        return (new_params, new_sched_state, step_count + 1), (\n            val + g_val,\n            grad_map_norm,\n        )\n\n    def epoch_fn(state: ProxGDState, _):\n        def run_step(s: ProxGDState):\n            (new_params, new_sched, new_step), (total_val, gm_norm) = step_fn(\n                (s.params, s.schedule_state, s.step)\n            )\n            is_conv = gm_norm &lt; tol_val\n            return ProxGDState(\n                params=new_params,\n                schedule_state=new_sched,\n                step=new_step,\n                value=total_val,\n                converged=is_conv,\n            )\n\n        def skip_step(s: ProxGDState):\n            return s\n\n        new_state = jax.lax.cond(state.converged, skip_step, run_step, state)\n\n        if verbose:\n            jax.debug.print(\"Epoch {}: value={}\", new_state.step, new_state.value)\n\n        return new_state, new_state.value\n\n    def step_fn_no_tol(carry):\n        params, sched_state, step_count = carry\n        lr_val, new_sched_state = scheduler(step_count, sched_state)\n\n        val, grads = jax.value_and_grad(fun)(params, *data)\n        g_val = g(params)\n\n        new_params = jax.tree.map(\n            lambda p, gr: prox(p - lr_val * gr, lr_val), params, grads\n        )\n\n        return (new_params, new_sched_state, step_count + 1), (val + g_val)\n\n    def epoch_fn_no_tol(state: ProxGDState, _):\n        (new_params, new_sched, new_step), total_val = step_fn_no_tol(\n            (state.params, state.schedule_state, state.step)\n        )\n        new_state = ProxGDState(\n            params=new_params,\n            schedule_state=new_sched,\n            step=new_step,\n            value=total_val,\n            converged=jnp.array(False, dtype=jnp.bool_),\n        )\n\n        if verbose:\n            jax.debug.print(\"Epoch {}: value={}\", new_state.step, new_state.value)\n\n        return new_state, new_state.value\n\n    if tol &lt;= 0.0:\n        final_state, trace = jax.lax.scan(\n            epoch_fn_no_tol, init_state, None, length=max_epochs\n        )\n    else:\n        final_state, trace = jax.lax.scan(epoch_fn, init_state, None, length=max_epochs)\n    final_value = fun(final_state.params, *data) + g(final_state.params)\n\n    return OptResult(\n        params=final_state.params,\n        final_value=final_value,\n        trace=trace,\n        success=final_state.converged,\n    )\n</code></pre>"},{"location":"solvers/#proximal-operators","title":"Proximal operators","text":"<p><code>prox_gd</code> expects an uncurried prox operator \\((x,\\eta)\\mapsto \\operatorname{prox}_{\\eta g}(x)\\). Two helpers are included:</p> <pre><code>from nano_optax import prox_l1, prox_l2\n\nprox_l1_op = prox_l1(reg=1.0)\nprox_l2_op = prox_l2(reg=0.1)\n</code></pre>"},{"location":"solvers/#nano_optax.prox_gd.prox_l1","title":"prox_l1","text":"<pre><code>prox_l1(\n    reg: float = 1.0,\n) -&gt; Callable[[Array, Array], Array]\n</code></pre> <p>Return the L1 proximal operator (soft-thresholding) as an uncurried map \\((x,\\eta)\\mapsto \\operatorname{prox}_{\\eta \\| \\cdot \\|_1}(x)\\).</p> Source code in <code>src/nano_optax/prox_gd.py</code> <pre><code>def prox_l1(reg: float = 1.0) -&gt; Callable[[jax.Array, jax.Array], jax.Array]:\n    r\"\"\"Return the L1 proximal operator (soft-thresholding) as an uncurried map\n    $(x,\\eta)\\mapsto \\operatorname{prox}_{\\eta \\| \\cdot \\|_1}(x)$. \"\"\"\n    if reg &lt; 0:\n        raise ValueError(\"Regularization coefficient must be nonnegative.\")\n    return lambda x, lr: jnp.sign(x) * jnp.maximum(0, jnp.abs(x) - reg * lr)\n</code></pre>"},{"location":"solvers/#nano_optax.prox_gd.prox_l2","title":"prox_l2","text":"<pre><code>prox_l2(\n    reg: float = 1.0,\n) -&gt; Callable[[Array, Array], Array]\n</code></pre> <p>Return the squared-L2 norm's proximal operator as an uncurried map \\((x,\\eta)\\mapsto \\operatorname{prox}_{\\eta \\| \\cdot \\|_{2}^{2}}(x)\\).</p> Source code in <code>src/nano_optax/prox_gd.py</code> <pre><code>def prox_l2(reg: float = 1.0) -&gt; Callable[[jax.Array, jax.Array], jax.Array]:\n    r\"\"\"Return the squared-L2 norm's proximal operator as an uncurried map\n    $(x,\\eta)\\mapsto \\operatorname{prox}_{\\eta \\| \\cdot \\|_{2}^{2}}(x)$.\"\"\"\n    if reg &lt; 0:\n        raise ValueError(\"Regularization coefficient must be nonnegative.\")\n    return lambda x, lr: x / (1 + (2 * reg * lr))\n</code></pre>"},{"location":"solvers/#accelerated-proximal-gradient-descent-fista","title":"Accelerated Proximal Gradient Descent (FISTA)","text":""},{"location":"solvers/#nano_optax.apgd.apgd","title":"apgd","text":"<pre><code>apgd(\n    fun: Callable[..., Array],\n    g: Callable[[PyTree], Array],\n    prox: Callable[[Array, Array], Array],\n    init_params: PyTree,\n    data: tuple[Array, ...],\n    *,\n    lr: LearningRate = 0.001,\n    max_epochs: int = 100,\n    batch_size: int | None = None,\n    key: Array | None = None,\n    tol: float = 1e-06,\n    schedule_state: ScheduleState | None = None,\n    verbose: bool = False,\n) -&gt; OptResult\n</code></pre> <p>Run accelerated proximal gradient descent (FISTA).</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable[..., Array]</code> <p>Smooth function <code>f(params, *batch_data) -&gt; value</code>.</p> required <code>g</code> <code>Callable[[PyTree], Array]</code> <p>Nonsmooth function <code>g(params) -&gt; value</code>.</p> required <code>prox</code> <code>Callable[[Array, Array], Array]</code> <p>Proximal operator <code>prox(params, lr) -&gt; params</code>.</p> required <code>init_params</code> <code>PyTree</code> <p>Initial parameters (PyTree).</p> required <code>data</code> <code>tuple[Array, ...]</code> <p>Tuple of data arrays, sliced along axis 0.</p> required <code>lr</code> <code>LearningRate</code> <p>Learning rate (constant, schedule, or stateful schedule).</p> <code>0.001</code> <code>max_epochs</code> <code>int</code> <p>Number of epochs to run.</p> <code>100</code> <code>batch_size</code> <code>int | None</code> <p>Minibatch size (None uses full batch).</p> <code>None</code> <code>key</code> <code>Array | None</code> <p>PRNGKey for shuffling (None disables shuffling).</p> <code>None</code> <code>tol</code> <code>float</code> <p>Convergence tolerance on gradient mapping norm.</p> <code>1e-06</code> <code>schedule_state</code> <code>ScheduleState | None</code> <p>Optional initial state for a stateful schedule.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress during optimization.</p> <code>False</code> <p>Returns:</p> Type Description <code>OptResult</code> <p>OptResult with final parameters, value, and trace.</p> Source code in <code>src/nano_optax/apgd.py</code> <pre><code>def apgd(\n    fun: Callable[..., jax.Array],\n    g: Callable[[PyTree], jax.Array],\n    prox: Callable[[jax.Array, jax.Array], jax.Array],\n    init_params: PyTree,\n    data: tuple[jax.Array, ...],\n    *,\n    lr: LearningRate = 1e-3,\n    max_epochs: int = 100,\n    batch_size: int | None = None,\n    key: jax.Array | None = None,\n    tol: float = 1e-6,\n    schedule_state: ScheduleState | None = None,\n    verbose: bool = False,\n) -&gt; OptResult:\n    \"\"\"Run accelerated proximal gradient descent (FISTA).\n\n    Args:\n        fun: Smooth function `f(params, *batch_data) -&gt; value`.\n        g: Nonsmooth function `g(params) -&gt; value`.\n        prox: Proximal operator `prox(params, lr) -&gt; params`.\n        init_params: Initial parameters (PyTree).\n        data: Tuple of data arrays, sliced along axis 0.\n        lr: Learning rate (constant, schedule, or stateful schedule).\n        max_epochs: Number of epochs to run.\n        batch_size: Minibatch size (None uses full batch).\n        key: PRNGKey for shuffling (None disables shuffling).\n        tol: Convergence tolerance on gradient mapping norm.\n        schedule_state: Optional initial state for a stateful schedule.\n        verbose: Print progress during optimization.\n\n    Returns:\n        OptResult with final parameters, value, and trace.\n    \"\"\"\n    if not data:\n        raise ValueError(\"data cannot be empty for APGD.\")\n\n    num_samples = len(data[0])\n    batch_size = num_samples if batch_size is None else min(batch_size, num_samples)\n    if batch_size &lt;= 0:\n        raise ValueError(\"batch_size must be positive\")\n\n    num_full_batches = num_samples // batch_size\n    remainder = num_samples % batch_size\n\n    scheduler, schedule_state = as_schedule(lr, schedule_state)\n    tol_val = jnp.asarray(tol)\n\n    init_state = APGDState(\n        params=init_params,\n        prev_params=init_params,\n        mom_t=jnp.array(1.0),\n        schedule_state=schedule_state,\n        step=jnp.array(0, dtype=jnp.int32),\n        key=key,\n        value=jnp.array(jnp.inf),\n        converged=jnp.array(False, dtype=jnp.bool_),\n    )\n\n    def step_fn(carry, indices):\n        params, prev_params, mom_t, sched_state, step_count = carry\n\n        next_t = (1.0 + jnp.sqrt(1.0 + 4.0 * mom_t**2)) / 2.0\n        beta = (mom_t - 1.0) / next_t\n\n        y_params = jax.tree.map(lambda p, pp: p + beta * (p - pp), params, prev_params)\n\n        batch_data = jax.tree.map(lambda x: x[indices], data)\n        lr_val, new_sched_state = scheduler(step_count, sched_state)\n\n        batch_val, grads = jax.value_and_grad(fun)(y_params, *batch_data)\n        new_params = jax.tree.map(\n            lambda y, gr: prox(y - lr_val * gr, lr_val), y_params, grads\n        )\n\n        g_val = g(y_params)\n\n        gm_sq = jax.tree_util.tree_reduce(\n            jnp.add,\n            jax.tree.map(\n                lambda y, np: jnp.sum(((y - np) / lr_val) ** 2),\n                y_params,\n                new_params,\n            ),\n        )\n        gm_norm = jnp.sqrt(gm_sq)\n\n        return (new_params, params, next_t, new_sched_state, step_count + 1), (\n            batch_val + g_val,\n            gm_norm,\n        )\n\n    def epoch_scan(carry: APGDState, _):\n        (\n            params,\n            prev_params,\n            mom_t,\n            sched_state,\n            step,\n            rng_key,\n            prev_epoch_val,\n            converged,\n        ) = carry\n\n        def run_epoch(operand):\n            p, prev_p, m_t, s_state, s, k = operand\n\n            if k is not None:\n                new_k, subkey = jax.random.split(k)\n                perm = jax.random.permutation(subkey, num_samples)\n            else:\n                new_k = k\n                perm = jnp.arange(num_samples)\n\n            weighted_sum = jnp.array(0.0)\n            accum_gm_norm = jnp.array(0.0)\n            count_batches = jnp.array(0.0)\n\n            scan_carry = (p, prev_p, m_t, s_state, s)\n\n            if num_full_batches &gt; 0:\n                full_indices = perm[: num_full_batches * batch_size].reshape(\n                    (num_full_batches, batch_size)\n                )\n                scan_carry, (batch_vals, batch_gms) = jax.lax.scan(\n                    step_fn, scan_carry, full_indices\n                )\n                weighted_sum += jnp.sum(batch_vals) * batch_size\n                accum_gm_norm += jnp.sum(batch_gms)\n                count_batches += num_full_batches\n\n            if remainder &gt; 0:\n                rem_indices = perm[num_full_batches * batch_size :]\n                scan_carry, (val, gm) = step_fn(scan_carry, rem_indices)\n                weighted_sum += val * remainder\n                accum_gm_norm += gm\n                count_batches += 1\n\n            new_p, new_prev_p, new_m_t, new_s_state, new_s = scan_carry\n\n            epoch_val = weighted_sum / num_samples\n            avg_gm_norm = accum_gm_norm / count_batches\n\n            return (\n                new_p,\n                new_prev_p,\n                new_m_t,\n                new_s_state,\n                new_s,\n                new_k,\n                epoch_val,\n                avg_gm_norm,\n            )\n\n        def skip_epoch(operand):\n            p, prev_p, m_t, s_state, s, k = operand\n            return p, prev_p, m_t, s_state, s, k, prev_epoch_val, jnp.array(0.0)\n\n        (\n            new_params,\n            new_prev_params,\n            new_mom_t,\n            new_sched_state,\n            new_step,\n            new_key,\n            epoch_val,\n            epoch_gm_norm,\n        ) = jax.lax.cond(\n            converged,\n            skip_epoch,\n            run_epoch,\n            (params, prev_params, mom_t, sched_state, step, rng_key),\n        )\n\n        is_conv = epoch_gm_norm &lt; tol_val\n        now_converged = jnp.logical_or(converged, is_conv)\n\n        new_state = APGDState(\n            params=new_params,\n            prev_params=new_prev_params,\n            mom_t=new_mom_t,\n            schedule_state=new_sched_state,\n            step=new_step,\n            key=new_key,\n            value=epoch_val,\n            converged=now_converged,\n        )\n\n        if verbose:\n            jax.debug.print(\"Epoch {}: value={}\", new_step, epoch_val)\n\n        return new_state, epoch_val\n\n    final_state, trace = jax.lax.scan(epoch_scan, init_state, None, length=max_epochs)\n    final_value = fun(final_state.params, *data) + g(final_state.params)\n\n    return OptResult(\n        params=final_state.params,\n        final_value=final_value,\n        trace=trace,\n        success=final_state.converged,\n    )\n</code></pre>"}]}